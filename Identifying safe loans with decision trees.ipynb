{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import data\n",
    "loans = pd.read_csv('lending-club-data.csv',dtype = {'desc':np.str,'next_pymnt_d':np.str})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In order to make this more intuitive and consistent with the lectures, we reassign the \n",
    "# target to be:\n",
    "\n",
    "loans['safe_loans'] = loans['bad_loans'].apply(lambda x: +1 if x == 0 else -1 )\n",
    "\n",
    "# only use a subset of the features\n",
    "\n",
    "features = ['grade',                     # grade of the loan\n",
    "            'sub_grade',                 # sub-grade of the loan\n",
    "            'short_emp',                 # one year or less of employment\n",
    "            'emp_length_num',            # number of years of employment\n",
    "            'home_ownership',            # home_ownership status: own, mortgage or rent\n",
    "            'dti',                       # debt to income ratio\n",
    "            'purpose',                   # the purpose of the loan\n",
    "            'term',                      # the term of the loan\n",
    "            'last_delinq_none',          # has borrower had a delinquincy\n",
    "            'last_major_derog_none',     # has borrower had 90 day or worse rating\n",
    "            'revol_util',                # percent of available credit being used\n",
    "            'total_rec_late_fee',        # total late fees received to day\n",
    "           ]\n",
    "\n",
    "target = 'safe_loans'                    # prediction target (y) (+1 means safe, -1 is risky)\n",
    "\n",
    "# Extract the feature columns and target column\n",
    "loans = loans[features + [target]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split into training and validation\n",
    "\n",
    "import json \n",
    "with open('module-5-assignment-1-validation-idx.json') as valid_index:\n",
    "    valid_index = json.load(valid_index)\n",
    "with open('module-5-assignment-1-train-idx.json') as train_index:\n",
    "    train_index = json.load(train_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categorical_variables = []\n",
    "for feat_name,feat_type in zip(list(loans.columns),list(loans.dtypes)):\n",
    "    if feat_type == \"object\":\n",
    "        categorical_variables.append(feat_name)\n",
    "        \n",
    "for feature in categorical_variables:\n",
    "    globals()['df_'+feature] = pd.get_dummies(loans[feature])\n",
    "    globals()['df_'+feature].columns = [feature+'_'+str(col) for col in  \\\n",
    "                                        globals()['df_'+feature].columns]\n",
    "    loans.pop(feature)\n",
    "    loans = pd.concat([loans,globals()['df_'+feature]],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "safe_loans_raw = loans[loans['safe_loans'] == 1]\n",
    "risky_loans_raw = loans[loans['safe_loans'] == -1]\n",
    "\n",
    "print (\"Number of safe loans: %s\" %(len(safe_loans_raw)))\n",
    "print (\"Number of safe loans: %s\" %(len(risky_loans_raw)))\n",
    "\n",
    "\n",
    "### One way to combat class imbalance is to undersample the larger class until the class distribution is approximately half and half. Here, we will undersample the larger class (safe loans) in order to balance out our dataset. This means we are throwing away many data points. We used seed=1 so everyone gets the same results.\n",
    "\n",
    "Since there are fewer risky loans than safe loans, find the ratio of the sizes\n",
    "and use that percentage to undersample the safe loans.\n",
    "percentage = len(risky_loans_raw)/float(len(safe_loans_raw))\n",
    "\n",
    "risky_loans = risky_loans_raw\n",
    "safe_loans = safe_loans_raw.sample(frac = percentage, random_state=1)\n",
    "\n",
    "Append the risky_loans with the downsampled version of safe_loans\n",
    "loans_data = risky_loans.append(safe_loans)\n",
    "\n",
    "\n",
    "### Create dummy variables for categorical variables\n",
    "\n",
    "categorical_variables = []\n",
    "for feat_name,feat_type in zip(list(loans_data.columns),list(loans_data.dtypes)):\n",
    "    if feat_type == \"object\":\n",
    "        categorical_variables.append(feat_name)\n",
    "        \n",
    "for feature in categorical_variables:\n",
    "    globals()['df_'+feature] = pd.get_dummies(loans_data[feature])\n",
    "    globals()['df_'+feature].columns = [feature+'_'+str(col) for col in  \\\n",
    "                                        globals()['df_'+feature].columns]\n",
    "    deleted_col = loans_data.pop(feature)\n",
    "    loans_data = pd.concat([loans_data,globals()['df_'+feature]],axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = loans.iloc[train_index]\n",
    "validation_data = loans.iloc[valid_index]\n",
    "\n",
    "model_feature = list(train_data.columns)\n",
    "model_feature.remove('safe_loans')\n",
    "\n",
    "x_train = train_data[model_feature].values\n",
    "y_train = train_data['safe_loans'].values\n",
    "\n",
    "x_valid = validation_data[model_feature].values\n",
    "y_valid = validation_data['safe_loans'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier_2 = DecisionTreeClassifier(max_depth = 2)\n",
    "classifier_6 = DecisionTreeClassifier(max_depth = 6)\n",
    "\n",
    "decision_tree_model = classifier_6.fit(x_train,y_train)\n",
    "\n",
    "small_model = classifier_2.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validation_safe_loans = validation_data[validation_data[target] == 1]\n",
    "validation_risky_loans = validation_data[validation_data[target] == -1]\n",
    "\n",
    "sample_validation_data_risky = validation_risky_loans[0:2]\n",
    "sample_validation_data_safe = validation_safe_loans[0:2]\n",
    "\n",
    "sample_validation_data = sample_validation_data_safe.append(sample_validation_data_risky)\n",
    "\n",
    "x_sample_validation = sample_validation_data[model_feature].values\n",
    "\n",
    "y_sample_validation = sample_validation_data['safe_loans'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, -1, -1,  1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_tree_model.predict(x_sample_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, -1, -1,  1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_model.predict(x_sample_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.34156543,  0.65843457],\n",
       "       [ 0.53630646,  0.46369354],\n",
       "       [ 0.64750958,  0.35249042],\n",
       "       [ 0.20789474,  0.79210526]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_tree_model.predict_proba(x_sample_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.41896585,  0.58103415],\n",
       "       [ 0.59255339,  0.40744661],\n",
       "       [ 0.59255339,  0.40744661],\n",
       "       [ 0.23120112,  0.76879888]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_model.predict_proba(x_sample_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3013, 1661],\n",
       "       [1717, 2893]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_valid,decision_tree_model.predict(x_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3342, 1332],\n",
       "       [2202, 2408]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_valid,small_model.predict(x_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6361482119775959"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(3013+2893)/float(3013+1661+1717+2893)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier_10 = DecisionTreeClassifier(max_depth = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "big_model = classifier_10.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3024, 1650],\n",
       "       [1823, 2787]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_valid,big_model.predict(x_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
